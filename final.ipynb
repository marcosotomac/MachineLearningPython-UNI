{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNoLpShC6mSJ7Gyar4Dt2mf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Submodulo metrics de sklearn\n","\n"],"metadata":{"id":"w5GM8Dtbvw7m"}},{"cell_type":"markdown","source":["## Metricas para evaluar problemas de Regresion\n","\n","**Error Cuadratico Medio** (Mean Squared Error : MSE )\n","\n","$$\n","\\frac{1}{n}\\sum_{i = 1}^{n} (y_i  - \\hat{y_i})^2\n","$$\n","\n","```python\n","from sklearn import metrics\n","metrics.mean_squared_error(y_true, y_pred)\n","```\n","\n","**Raiz del Error cuadratico medio** (Root Mean Squared Error : RMSE)\n","\n","$$\n","\\sqrt{\\frac{1}{n}\\sum_{i = 1}^{n} (y_i  - \\hat{y_i})^2}\n","$$\n","\n","```\n","metrics.mean_squared_error(y_true, y_pred, squared = False)\n","```\n","\n","**Error Absoluto Medio** (Mean Squared Error :MAE)\n","$$\n","\\frac{1}{n} \\sum_{i = 1}^{n} |y_i - \\hat{y_i}|\n","$$\n","\n","\n","```\n","metrics.mean_absolute_error(y_true, y_pred)\n","```\n","\n","**Error Absoluto Medio Porcentual** (Mean Absolute Percentage Error : MAPE)\n","$$\n","\\frac{100\\text{%}}{n} \\sum_{i=1}^{n} \\left| \\frac{y_i - \\hat{y_i}}{y_i} \\right|\n","$$\n","\n","\n","\n","```\n","metrics.mean_absolute_percentage_error(y_true, y_pred)\n","```\n","\n","\n","### Otros indicadores de calidad\n","  > Coeficiente de determinacion ($R^2$)\n","\n","  > Error Cuadratico Medio Logaritmico\n","\n","  > Maximo Error\n","\n","\n","\n","\n","\n"],"metadata":{"id":"-oMntx9qwThW"}},{"cell_type":"markdown","source":["## Consideraciones Estadisticas y Computacionales\n","\n","### Robustez y Sensibilidad\n","  * MAE y MAPE : Robustos a outliers\n","  * MSE/RMSE : Sensibles a outliers (utiles cuando los errores grandes son criticos)\n","\n","### Escalabilidad\n","  * Todas las metricas son computacionalmente eficientes\n","  * Implementaciones vectorizadas"],"metadata":{"id":"O0ybT49g0u5e"}},{"cell_type":"markdown","source":["# Modelos de Regresion  Lineal : submodulo linear_model"],"metadata":{"id":"LcRpE6Nj2tWu"}},{"cell_type":"code","source":["# Importamos la clase a modelar\n","from sklearn.linear_model import LinearRegression\n","help(LinearRegression)"],"metadata":{"collapsed":true,"id":"t0wyaos721Uu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Estrategia para obtener un modelo de regresion lineal\n","\n","* Definicion de $X$ (todas las variables independientes) e $y$ (variable dependiente)\n","\n","* Particionar los datos en un subconjunto de entrenamiento (ajustar el modelo) y un subconjunto de testeo (calcular indicadores/metricas para medir el poder predictivo del modelo)\n","\n","* Instanciar la clase a modelar (LinearRegression)\n","\n","* Ajustar la clase a modelar usando el subconjunto  de entrenamiento (Paso2)\n","\n","* COnstruimos pronosticos de la variable dependiente a partir del modelo y del subconjunto de testeo de las variables independientes\n","\n","* Calculamos un indicador de calidad (metrica)."],"metadata":{"id":"kgIT-alH3PoW"}},{"cell_type":"markdown","source":["# Analizar a los modelos que compone al submodulo `linear_model`\n","\n","\n","\n","```\n","import sklear.linear_model\n","dir(sklear.linear_model)\n","```\n","\n"],"metadata":{"id":"rRic5W_B486A"}},{"cell_type":"code","source":["# 'ARDRegression',\n","#  'BayesianRidge',\n","#  'ElasticNet',\n","#  'ElasticNetCV',\n","#  'GammaRegressor',\n","#  'HuberRegressor',\n","#  'Lars',\n","#  'LarsCV',\n","#  'Lasso',\n","#  'LassoCV',\n","#  'LassoLars',\n","#  'LassoLarsCV',\n","#  'LassoLarsIC',\n","#  'LinearRegression',\n","#  'LogisticRegression',\n","#  'LogisticRegressionCV',\n","#  'MultiTaskElasticNet',\n","#  'MultiTaskElasticNetCV',\n","#  'MultiTaskLasso',\n","#  'MultiTaskLassoCV',\n","#  'OrthogonalMatchingPursuit',\n","#  'OrthogonalMatchingPursuitCV',\n","#  'PassiveAggressiveClassifier',\n","#  'PassiveAggressiveRegressor',\n","#  'Perceptron',\n","#  'PoissonRegressor',\n","#  'QuantileRegressor',\n","#  'RANSACRegressor',\n","#  'Ridge',\n","#  'RidgeCV',\n","#  'RidgeClassifier',\n","#  'RidgeClassifierCV',\n","#  'SGDClassifier',\n","#  'SGDOneClassSVM',\n","#  'SGDRegressor',\n","#  'TheilSenRegressor',\n","#  'TweedieRegressor',"],"metadata":{"id":"8tuZD5fk5Wo-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tecnicas de regularizacion\n","\n","Las tecnicas de regularizacion es aÃ±adir un termino de penalizacion sobre los coeficientes del modelo\n","\n","* L1 (Lasso)\n","* L2 (Ridge)\n","* ElasticNet"],"metadata":{"id":"odMchvZ85mTh"}},{"cell_type":"markdown","source":["## Limitaciones de la utilizacion del metodo de Minimos Cuadrados (ordinarios)\n","\n","$$\n","\\min_{c_i} \\| y - XC \\|^2\n","$$"],"metadata":{"id":"IAqrXWG17DWP"}},{"cell_type":"markdown","source":["## Ventajas de los modelos regularizados\n","\n","* Regresion Lasso (L1)\n","\n","$$\n","\\min_{c_i} \\| y - XC \\|^2 + \\alpha \\|C\\|_1\n","$$\n","\n","* Regresion Ridge (L2)\n","\n","$$\n","\\min_{c_i} \\| y - XC \\|^2 + \\alpha \\|C\\|^2\n","$$\n","\n","\n","* ElasticNet\n","\n","$$\n","\\min_{C_0, C} \\left\\{ \\frac{1}{2n}  \\| y - XC - C_0 1 \\|^2 +\n","\\alpha\\rho \\|C\\| + \\frac{\\alpha(1-\\rho)}{2} \\|C\\|^2\n","\\right\\}\n","$$"],"metadata":{"id":"TYYaORru8uBX"}},{"cell_type":"code","source":["help(sklearn.linear_model.Lasso)"],"metadata":{"id":"bf2NhYoR_d2w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["help(sklearn.linear_model.Ridge)"],"metadata":{"id":"fP4Zl80P_pHw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["help(sklearn.linear_model.ElasticNet)"],"metadata":{"id":"hd3T3pbN_wSX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Modelos de Aprendizaje Supervisado para problemas de clasificacion : Regresion logistica Binomial\n","\n","Un modelo lineal generalizado se define mediante 3 componentes:\n","* Distribucion de la variable de respuesta\n","* Funcion de enlace\n","* Predictor Lineal\n","\n","> Un modelo de clasificacion basado en una regresion  logistica satisface estos tres componentes : una distribucion binomial y una funcion de enlace logic\n","\n"],"metadata":{"id":"W7tBfqhDAQRg"}},{"cell_type":"markdown","source":["## Formulacion matematica del modelo lineal Generalizacion para un problema de clasificacion binomial : $y \\in \\{0,1\\}$\n","\n","> Primera componente : $y_i \\sim $ Bernoulli($\\mu_i$)\n","\n","> Segundo Componente : Funcion de enlace logit\n","\n","$$\n","g(\\mu_i) = \\log \\left(  \\frac{\\mu_i}{1- \\mu_i}\\right) = x_i^{T} C\n","$$\n","\n","Definir la regresion logistica:\n","$$\n","\\log \\left( \\frac{\\mathbb{P} (y_1 =1 |x_i)}{1- \\mathbb{P} (y_1 =1 |x_i)}\\right) = c_0 + C_1 x_{i1} + C_2 x_{i2} + C_p x_{ip} = x_i^{T} C\n","$$\n","\n","$$\n","\\mathbb{P} (y_1 =1 |x_i) = \\frac{\\exp(x_i^{T} C)}{1-\\exp(x_i^{T} C)}\n","$$"],"metadata":{"id":"Al6-CJDMCy9Y"}},{"cell_type":"code","source":["#\n","from sklearn.linear_model import LogisticRegression\n","help(LogisticRegression)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"1o4taymPFRVp","executionInfo":{"status":"ok","timestamp":1755787389133,"user_tz":300,"elapsed":173,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"0a529440-5185-472c-b888-b7d443bb447e"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Help on class LogisticRegression in module sklearn.linear_model._logistic:\n","\n","class LogisticRegression(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n"," |  LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='deprecated', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n"," |\n"," |  Logistic Regression (aka logit, MaxEnt) classifier.\n"," |\n"," |  This class implements regularized logistic regression using the\n"," |  'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note\n"," |  that regularization is applied by default**. It can handle both dense\n"," |  and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit\n"," |  floats for optimal performance; any other input format will be converted\n"," |  (and copied).\n"," |\n"," |  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization\n"," |  with primal formulation, or no regularization. The 'liblinear' solver\n"," |  supports both L1 and L2 regularization, with a dual formulation only for\n"," |  the L2 penalty. The Elastic-Net regularization is only supported by the\n"," |  'saga' solver.\n"," |\n"," |  For :term:`multiclass` problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'\n"," |  handle multinomial loss. 'liblinear' and 'newton-cholesky' only handle binary\n"," |  classification but can be extended to handle multiclass by using\n"," |  :class:`~sklearn.multiclass.OneVsRestClassifier`.\n"," |\n"," |  Read more in the :ref:`User Guide <logistic_regression>`.\n"," |\n"," |  Parameters\n"," |  ----------\n"," |  penalty : {'l1', 'l2', 'elasticnet', None}, default='l2'\n"," |      Specify the norm of the penalty:\n"," |\n"," |      - `None`: no penalty is added;\n"," |      - `'l2'`: add a L2 penalty term and it is the default choice;\n"," |      - `'l1'`: add a L1 penalty term;\n"," |      - `'elasticnet'`: both L1 and L2 penalty terms are added.\n"," |\n"," |      .. warning::\n"," |         Some penalties may not work with some solvers. See the parameter\n"," |         `solver` below, to know the compatibility between the penalty and\n"," |         solver.\n"," |\n"," |      .. versionadded:: 0.19\n"," |         l1 penalty with SAGA solver (allowing 'multinomial' + L1)\n"," |\n"," |  dual : bool, default=False\n"," |      Dual (constrained) or primal (regularized, see also\n"," |      :ref:`this equation <regularized-logistic-loss>`) formulation. Dual formulation\n"," |      is only implemented for l2 penalty with liblinear solver. Prefer dual=False when\n"," |      n_samples > n_features.\n"," |\n"," |  tol : float, default=1e-4\n"," |      Tolerance for stopping criteria.\n"," |\n"," |  C : float, default=1.0\n"," |      Inverse of regularization strength; must be a positive float.\n"," |      Like in support vector machines, smaller values specify stronger\n"," |      regularization.\n"," |\n"," |  fit_intercept : bool, default=True\n"," |      Specifies if a constant (a.k.a. bias or intercept) should be\n"," |      added to the decision function.\n"," |\n"," |  intercept_scaling : float, default=1\n"," |      Useful only when the solver 'liblinear' is used\n"," |      and self.fit_intercept is set to True. In this case, x becomes\n"," |      [x, self.intercept_scaling],\n"," |      i.e. a \"synthetic\" feature with constant value equal to\n"," |      intercept_scaling is appended to the instance vector.\n"," |      The intercept becomes ``intercept_scaling * synthetic_feature_weight``.\n"," |\n"," |      Note! the synthetic feature weight is subject to l1/l2 regularization\n"," |      as all other features.\n"," |      To lessen the effect of regularization on synthetic feature weight\n"," |      (and therefore on the intercept) intercept_scaling has to be increased.\n"," |\n"," |  class_weight : dict or 'balanced', default=None\n"," |      Weights associated with classes in the form ``{class_label: weight}``.\n"," |      If not given, all classes are supposed to have weight one.\n"," |\n"," |      The \"balanced\" mode uses the values of y to automatically adjust\n"," |      weights inversely proportional to class frequencies in the input data\n"," |      as ``n_samples / (n_classes * np.bincount(y))``.\n"," |\n"," |      Note that these weights will be multiplied with sample_weight (passed\n"," |      through the fit method) if sample_weight is specified.\n"," |\n"," |      .. versionadded:: 0.17\n"," |         *class_weight='balanced'*\n"," |\n"," |  random_state : int, RandomState instance, default=None\n"," |      Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle the\n"," |      data. See :term:`Glossary <random_state>` for details.\n"," |\n"," |  solver : {'lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'},             default='lbfgs'\n"," |\n"," |      Algorithm to use in the optimization problem. Default is 'lbfgs'.\n"," |      To choose a solver, you might want to consider the following aspects:\n"," |\n"," |      - For small datasets, 'liblinear' is a good choice, whereas 'sag'\n"," |        and 'saga' are faster for large ones;\n"," |      - For :term:`multiclass` problems, all solvers except 'liblinear' minimize the\n"," |        full multinomial loss;\n"," |      - 'liblinear' can only handle binary classification by default. To apply a\n"," |        one-versus-rest scheme for the multiclass setting one can wrap it with the\n"," |        :class:`~sklearn.multiclass.OneVsRestClassifier`.\n"," |      - 'newton-cholesky' is a good choice for\n"," |        `n_samples` >> `n_features * n_classes`, especially with one-hot encoded\n"," |        categorical features with rare categories. Be aware that the memory usage\n"," |        of this solver has a quadratic dependency on `n_features * n_classes`\n"," |        because it explicitly computes the full Hessian matrix.\n"," |\n"," |      .. warning::\n"," |         The choice of the algorithm depends on the penalty chosen and on\n"," |         (multinomial) multiclass support:\n"," |\n"," |         ================= ============================== ======================\n"," |         solver            penalty                        multinomial multiclass\n"," |         ================= ============================== ======================\n"," |         'lbfgs'           'l2', None                     yes\n"," |         'liblinear'       'l1', 'l2'                     no\n"," |         'newton-cg'       'l2', None                     yes\n"," |         'newton-cholesky' 'l2', None                     no\n"," |         'sag'             'l2', None                     yes\n"," |         'saga'            'elasticnet', 'l1', 'l2', None yes\n"," |         ================= ============================== ======================\n"," |\n"," |      .. note::\n"," |         'sag' and 'saga' fast convergence is only guaranteed on features\n"," |         with approximately the same scale. You can preprocess the data with\n"," |         a scaler from :mod:`sklearn.preprocessing`.\n"," |\n"," |      .. seealso::\n"," |         Refer to the :ref:`User Guide <Logistic_regression>` for more\n"," |         information regarding :class:`LogisticRegression` and more specifically the\n"," |         :ref:`Table <logistic_regression_solvers>`\n"," |         summarizing solver/penalty supports.\n"," |\n"," |      .. versionadded:: 0.17\n"," |         Stochastic Average Gradient descent solver.\n"," |      .. versionadded:: 0.19\n"," |         SAGA solver.\n"," |      .. versionchanged:: 0.22\n"," |          The default solver changed from 'liblinear' to 'lbfgs' in 0.22.\n"," |      .. versionadded:: 1.2\n"," |         newton-cholesky solver.\n"," |\n"," |  max_iter : int, default=100\n"," |      Maximum number of iterations taken for the solvers to converge.\n"," |\n"," |  multi_class : {'auto', 'ovr', 'multinomial'}, default='auto'\n"," |      If the option chosen is 'ovr', then a binary problem is fit for each\n"," |      label. For 'multinomial' the loss minimised is the multinomial loss fit\n"," |      across the entire probability distribution, *even when the data is\n"," |      binary*. 'multinomial' is unavailable when solver='liblinear'.\n"," |      'auto' selects 'ovr' if the data is binary, or if solver='liblinear',\n"," |      and otherwise selects 'multinomial'.\n"," |\n"," |      .. versionadded:: 0.18\n"," |         Stochastic Average Gradient descent solver for 'multinomial' case.\n"," |      .. versionchanged:: 0.22\n"," |          Default changed from 'ovr' to 'auto' in 0.22.\n"," |      .. deprecated:: 1.5\n"," |         ``multi_class`` was deprecated in version 1.5 and will be removed in 1.7.\n"," |         From then on, the recommended 'multinomial' will always be used for\n"," |         `n_classes >= 3`.\n"," |         Solvers that do not support 'multinomial' will raise an error.\n"," |         Use `sklearn.multiclass.OneVsRestClassifier(LogisticRegression())` if you\n"," |         still want to use OvR.\n"," |\n"," |  verbose : int, default=0\n"," |      For the liblinear and lbfgs solvers set verbose to any positive\n"," |      number for verbosity.\n"," |\n"," |  warm_start : bool, default=False\n"," |      When set to True, reuse the solution of the previous call to fit as\n"," |      initialization, otherwise, just erase the previous solution.\n"," |      Useless for liblinear solver. See :term:`the Glossary <warm_start>`.\n"," |\n"," |      .. versionadded:: 0.17\n"," |         *warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.\n"," |\n"," |  n_jobs : int, default=None\n"," |      Number of CPU cores used when parallelizing over classes if\n"," |      multi_class='ovr'\". This parameter is ignored when the ``solver`` is\n"," |      set to 'liblinear' regardless of whether 'multi_class' is specified or\n"," |      not. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n"," |      context. ``-1`` means using all processors.\n"," |      See :term:`Glossary <n_jobs>` for more details.\n"," |\n"," |  l1_ratio : float, default=None\n"," |      The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Only\n"," |      used if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalent\n"," |      to using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalent\n"," |      to using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a\n"," |      combination of L1 and L2.\n"," |\n"," |  Attributes\n"," |  ----------\n"," |\n"," |  classes_ : ndarray of shape (n_classes, )\n"," |      A list of class labels known to the classifier.\n"," |\n"," |  coef_ : ndarray of shape (1, n_features) or (n_classes, n_features)\n"," |      Coefficient of the features in the decision function.\n"," |\n"," |      `coef_` is of shape (1, n_features) when the given problem is binary.\n"," |      In particular, when `multi_class='multinomial'`, `coef_` corresponds\n"," |      to outcome 1 (True) and `-coef_` corresponds to outcome 0 (False).\n"," |\n"," |  intercept_ : ndarray of shape (1,) or (n_classes,)\n"," |      Intercept (a.k.a. bias) added to the decision function.\n"," |\n"," |      If `fit_intercept` is set to False, the intercept is set to zero.\n"," |      `intercept_` is of shape (1,) when the given problem is binary.\n"," |      In particular, when `multi_class='multinomial'`, `intercept_`\n"," |      corresponds to outcome 1 (True) and `-intercept_` corresponds to\n"," |      outcome 0 (False).\n"," |\n"," |  n_features_in_ : int\n"," |      Number of features seen during :term:`fit`.\n"," |\n"," |      .. versionadded:: 0.24\n"," |\n"," |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n"," |      Names of features seen during :term:`fit`. Defined only when `X`\n"," |      has feature names that are all strings.\n"," |\n"," |      .. versionadded:: 1.0\n"," |\n"," |  n_iter_ : ndarray of shape (n_classes,) or (1, )\n"," |      Actual number of iterations for all classes. If binary or multinomial,\n"," |      it returns only 1 element. For liblinear solver, only the maximum\n"," |      number of iteration across all classes is given.\n"," |\n"," |      .. versionchanged:: 0.20\n"," |\n"," |          In SciPy <= 1.0.0 the number of lbfgs iterations may exceed\n"," |          ``max_iter``. ``n_iter_`` will now report at most ``max_iter``.\n"," |\n"," |  See Also\n"," |  --------\n"," |  SGDClassifier : Incrementally trained logistic regression (when given\n"," |      the parameter ``loss=\"log_loss\"``).\n"," |  LogisticRegressionCV : Logistic regression with built-in cross validation.\n"," |\n"," |  Notes\n"," |  -----\n"," |  The underlying C implementation uses a random number generator to\n"," |  select features when fitting the model. It is thus not uncommon,\n"," |  to have slightly different results for the same input data. If\n"," |  that happens, try with a smaller tol parameter.\n"," |\n"," |  Predict output may not match that of standalone liblinear in certain\n"," |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n"," |  in the narrative documentation.\n"," |\n"," |  References\n"," |  ----------\n"," |\n"," |  L-BFGS-B -- Software for Large-scale Bound-constrained Optimization\n"," |      Ciyou Zhu, Richard Byrd, Jorge Nocedal and Jose Luis Morales.\n"," |      http://users.iems.northwestern.edu/~nocedal/lbfgsb.html\n"," |\n"," |  LIBLINEAR -- A Library for Large Linear Classification\n"," |      https://www.csie.ntu.edu.tw/~cjlin/liblinear/\n"," |\n"," |  SAG -- Mark Schmidt, Nicolas Le Roux, and Francis Bach\n"," |      Minimizing Finite Sums with the Stochastic Average Gradient\n"," |      https://hal.inria.fr/hal-00860051/document\n"," |\n"," |  SAGA -- Defazio, A., Bach F. & Lacoste-Julien S. (2014).\n"," |          :arxiv:`\"SAGA: A Fast Incremental Gradient Method With Support\n"," |          for Non-Strongly Convex Composite Objectives\" <1407.0202>`\n"," |\n"," |  Hsiang-Fu Yu, Fang-Lan Huang, Chih-Jen Lin (2011). Dual coordinate descent\n"," |      methods for logistic regression and maximum entropy models.\n"," |      Machine Learning 85(1-2):41-75.\n"," |      https://www.csie.ntu.edu.tw/~cjlin/papers/maxent_dual.pdf\n"," |\n"," |  Examples\n"," |  --------\n"," |  >>> from sklearn.datasets import load_iris\n"," |  >>> from sklearn.linear_model import LogisticRegression\n"," |  >>> X, y = load_iris(return_X_y=True)\n"," |  >>> clf = LogisticRegression(random_state=0).fit(X, y)\n"," |  >>> clf.predict(X[:2, :])\n"," |  array([0, 0])\n"," |  >>> clf.predict_proba(X[:2, :])\n"," |  array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n"," |         [9.7...e-01, 2.8...e-02, ...e-08]])\n"," |  >>> clf.score(X, y)\n"," |  0.97...\n"," |\n"," |  For a comaprison of the LogisticRegression with other classifiers see:\n"," |  :ref:`sphx_glr_auto_examples_classification_plot_classification_probability.py`.\n"," |\n"," |  Method resolution order:\n"," |      LogisticRegression\n"," |      sklearn.linear_model._base.LinearClassifierMixin\n"," |      sklearn.base.ClassifierMixin\n"," |      sklearn.linear_model._base.SparseCoefMixin\n"," |      sklearn.base.BaseEstimator\n"," |      sklearn.utils._estimator_html_repr._HTMLDocumentationLinkMixin\n"," |      sklearn.utils._metadata_requests._MetadataRequester\n"," |      builtins.object\n"," |\n"," |  Methods defined here:\n"," |\n"," |  __init__(self, penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='deprecated', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n"," |      Initialize self.  See help(type(self)) for accurate signature.\n"," |\n"," |  __sklearn_tags__(self)\n"," |\n"," |  fit(self, X, y, sample_weight=None)\n"," |      Fit the model according to the given training data.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          Training vector, where `n_samples` is the number of samples and\n"," |          `n_features` is the number of features.\n"," |\n"," |      y : array-like of shape (n_samples,)\n"," |          Target vector relative to X.\n"," |\n"," |      sample_weight : array-like of shape (n_samples,) default=None\n"," |          Array of weights that are assigned to individual samples.\n"," |          If not provided, then each sample is given unit weight.\n"," |\n"," |          .. versionadded:: 0.17\n"," |             *sample_weight* support to LogisticRegression.\n"," |\n"," |      Returns\n"," |      -------\n"," |      self\n"," |          Fitted estimator.\n"," |\n"," |      Notes\n"," |      -----\n"," |      The SAGA solver supports both float64 and float32 bit arrays.\n"," |\n"," |  predict_log_proba(self, X)\n"," |      Predict logarithm of probability estimates.\n"," |\n"," |      The returned estimates for all classes are ordered by the\n"," |      label of classes.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      X : array-like of shape (n_samples, n_features)\n"," |          Vector to be scored, where `n_samples` is the number of samples and\n"," |          `n_features` is the number of features.\n"," |\n"," |      Returns\n"," |      -------\n"," |      T : array-like of shape (n_samples, n_classes)\n"," |          Returns the log-probability of the sample for each class in the\n"," |          model, where classes are ordered as they are in ``self.classes_``.\n"," |\n"," |  predict_proba(self, X)\n"," |      Probability estimates.\n"," |\n"," |      The returned estimates for all classes are ordered by the\n"," |      label of classes.\n"," |\n"," |      For a multi_class problem, if multi_class is set to be \"multinomial\"\n"," |      the softmax function is used to find the predicted probability of\n"," |      each class.\n"," |      Else use a one-vs-rest approach, i.e. calculate the probability\n"," |      of each class assuming it to be positive using the logistic function\n"," |      and normalize these values across all the classes.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      X : array-like of shape (n_samples, n_features)\n"," |          Vector to be scored, where `n_samples` is the number of samples and\n"," |          `n_features` is the number of features.\n"," |\n"," |      Returns\n"," |      -------\n"," |      T : array-like of shape (n_samples, n_classes)\n"," |          Returns the probability of the sample for each class in the model,\n"," |          where classes are ordered as they are in ``self.classes_``.\n"," |\n"," |  set_fit_request(self: sklearn.linear_model._logistic.LogisticRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._logistic.LogisticRegression from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n"," |      Request metadata passed to the ``fit`` method.\n"," |\n"," |      Note that this method is only relevant if\n"," |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n"," |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |\n"," |      The options for each parameter are:\n"," |\n"," |      - ``True``: metadata is requested, and passed to ``fit`` if provided. The request is ignored if metadata is not provided.\n"," |\n"," |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``fit``.\n"," |\n"," |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n"," |\n"," |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n"," |\n"," |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n"," |      existing request. This allows you to change the request for some\n"," |      parameters and not others.\n"," |\n"," |      .. versionadded:: 1.3\n"," |\n"," |      .. note::\n"," |          This method is only relevant if this estimator is used as a\n"," |          sub-estimator of a meta-estimator, e.g. used inside a\n"," |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``sample_weight`` parameter in ``fit``.\n"," |\n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          The updated object.\n"," |\n"," |  set_score_request(self: sklearn.linear_model._logistic.LogisticRegression, *, sample_weight: Union[bool, NoneType, str] = '$UNCHANGED$') -> sklearn.linear_model._logistic.LogisticRegression from sklearn.utils._metadata_requests.RequestMethod.__get__.<locals>\n"," |      Request metadata passed to the ``score`` method.\n"," |\n"," |      Note that this method is only relevant if\n"," |      ``enable_metadata_routing=True`` (see :func:`sklearn.set_config`).\n"," |      Please see :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |\n"," |      The options for each parameter are:\n"," |\n"," |      - ``True``: metadata is requested, and passed to ``score`` if provided. The request is ignored if metadata is not provided.\n"," |\n"," |      - ``False``: metadata is not requested and the meta-estimator will not pass it to ``score``.\n"," |\n"," |      - ``None``: metadata is not requested, and the meta-estimator will raise an error if the user provides it.\n"," |\n"," |      - ``str``: metadata should be passed to the meta-estimator with this given alias instead of the original name.\n"," |\n"," |      The default (``sklearn.utils.metadata_routing.UNCHANGED``) retains the\n"," |      existing request. This allows you to change the request for some\n"," |      parameters and not others.\n"," |\n"," |      .. versionadded:: 1.3\n"," |\n"," |      .. note::\n"," |          This method is only relevant if this estimator is used as a\n"," |          sub-estimator of a meta-estimator, e.g. used inside a\n"," |          :class:`~sklearn.pipeline.Pipeline`. Otherwise it has no effect.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      sample_weight : str, True, False, or None,                     default=sklearn.utils.metadata_routing.UNCHANGED\n"," |          Metadata routing for ``sample_weight`` parameter in ``score``.\n"," |\n"," |      Returns\n"," |      -------\n"," |      self : object\n"," |          The updated object.\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Data and other attributes defined here:\n"," |\n"," |  __annotations__ = {'_parameter_constraints': <class 'dict'>}\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n"," |\n"," |  decision_function(self, X)\n"," |      Predict confidence scores for samples.\n"," |\n"," |      The confidence score for a sample is proportional to the signed\n"," |      distance of that sample to the hyperplane.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The data matrix for which we want to get the confidence scores.\n"," |\n"," |      Returns\n"," |      -------\n"," |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n"," |          Confidence scores per `(n_samples, n_classes)` combination. In the\n"," |          binary case, confidence score for `self.classes_[1]` where >0 means\n"," |          this class would be predicted.\n"," |\n"," |  predict(self, X)\n"," |      Predict class labels for samples in X.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n"," |          The data matrix for which we want to get the predictions.\n"," |\n"," |      Returns\n"," |      -------\n"," |      y_pred : ndarray of shape (n_samples,)\n"," |          Vector containing the class labels for each sample.\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.ClassifierMixin:\n"," |\n"," |  score(self, X, y, sample_weight=None)\n"," |      Return the mean accuracy on the given test data and labels.\n"," |\n"," |      In multi-label classification, this is the subset accuracy\n"," |      which is a harsh metric since you require for each sample that\n"," |      each label set be correctly predicted.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      X : array-like of shape (n_samples, n_features)\n"," |          Test samples.\n"," |\n"," |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n"," |          True labels for `X`.\n"," |\n"," |      sample_weight : array-like of shape (n_samples,), default=None\n"," |          Sample weights.\n"," |\n"," |      Returns\n"," |      -------\n"," |      score : float\n"," |          Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n"," |\n"," |  __dict__\n"," |      dictionary for instance variables\n"," |\n"," |  __weakref__\n"," |      list of weak references to the object\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n"," |\n"," |  densify(self)\n"," |      Convert coefficient matrix to dense array format.\n"," |\n"," |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n"," |      default format of ``coef_`` and is required for fitting, so calling\n"," |      this method is only required on models that have previously been\n"," |      sparsified; otherwise, it is a no-op.\n"," |\n"," |      Returns\n"," |      -------\n"," |      self\n"," |          Fitted estimator.\n"," |\n"," |  sparsify(self)\n"," |      Convert coefficient matrix to sparse format.\n"," |\n"," |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n"," |      L1-regularized models can be much more memory- and storage-efficient\n"," |      than the usual numpy.ndarray representation.\n"," |\n"," |      The ``intercept_`` member is not converted.\n"," |\n"," |      Returns\n"," |      -------\n"," |      self\n"," |          Fitted estimator.\n"," |\n"," |      Notes\n"," |      -----\n"," |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n"," |      this may actually *increase* memory usage, so use this method with\n"," |      care. A rule of thumb is that the number of zero elements, which can\n"," |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n"," |      to provide significant benefits.\n"," |\n"," |      After calling this method, further fitting with the partial_fit\n"," |      method (if any) will not work until you call densify.\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.base.BaseEstimator:\n"," |\n"," |  __getstate__(self)\n"," |      Helper for pickle.\n"," |\n"," |  __repr__(self, N_CHAR_MAX=700)\n"," |      Return repr(self).\n"," |\n"," |  __setstate__(self, state)\n"," |\n"," |  __sklearn_clone__(self)\n"," |\n"," |  get_params(self, deep=True)\n"," |      Get parameters for this estimator.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      deep : bool, default=True\n"," |          If True, will return the parameters for this estimator and\n"," |          contained subobjects that are estimators.\n"," |\n"," |      Returns\n"," |      -------\n"," |      params : dict\n"," |          Parameter names mapped to their values.\n"," |\n"," |  set_params(self, **params)\n"," |      Set the parameters of this estimator.\n"," |\n"," |      The method works on simple estimators as well as on nested objects\n"," |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n"," |      parameters of the form ``<component>__<parameter>`` so that it's\n"," |      possible to update each component of a nested object.\n"," |\n"," |      Parameters\n"," |      ----------\n"," |      **params : dict\n"," |          Estimator parameters.\n"," |\n"," |      Returns\n"," |      -------\n"," |      self : estimator instance\n"," |          Estimator instance.\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n"," |\n"," |  get_metadata_routing(self)\n"," |      Get metadata routing of this object.\n"," |\n"," |      Please check :ref:`User Guide <metadata_routing>` on how the routing\n"," |      mechanism works.\n"," |\n"," |      Returns\n"," |      -------\n"," |      routing : MetadataRequest\n"," |          A :class:`~sklearn.utils.metadata_routing.MetadataRequest` encapsulating\n"," |          routing information.\n"," |\n"," |  ----------------------------------------------------------------------\n"," |  Class methods inherited from sklearn.utils._metadata_requests._MetadataRequester:\n"," |\n"," |  __init_subclass__(**kwargs)\n"," |      Set the ``set_{method}_request`` methods.\n"," |\n"," |      This uses PEP-487 [1]_ to set the ``set_{method}_request`` methods. It\n"," |      looks for the information available in the set default values which are\n"," |      set using ``__metadata_request__*`` class attributes, or inferred\n"," |      from method signatures.\n"," |\n"," |      The ``__metadata_request__*`` class attributes are used when a method\n"," |      does not explicitly accept a metadata through its arguments or if the\n"," |      developer would like to specify a request value for those metadata\n"," |      which are different from the default ``None``.\n"," |\n"," |      References\n"," |      ----------\n"," |      .. [1] https://www.python.org/dev/peps/pep-0487\n","\n"]}]},{"cell_type":"markdown","source":["# Primer modelo de regresion logistica"],"metadata":{"id":"y_AwCuDjIF7i"}},{"cell_type":"code","source":["# Modulos y datos\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Aprendizaje Automatico / Machine Learning\n","from sklearn.model_selection import train_test_split\n","\n","# Regresion Logistica : Variable dependiente es categorica (Modelo de Clasificacion)\n","from sklearn.linear_model import LogisticRegression\n","\n","# Calculo de indicadores de calidad : Metricas\n","from sklearn import metrics\n","\n","# Dataset : Problema de clasificacion binaria\n","df = pd.read_csv(\"https://raw.githubusercontent.com/robintux/Datasets4StackOverFlowQuestions/refs/heads/master/breast-cancer-wisconsin.csv\")\n","df.info()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uRb0mlzbIIh4","executionInfo":{"status":"ok","timestamp":1755788444612,"user_tz":300,"elapsed":1081,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"d429a0f9-7263-4166-8bf5-a131b1d9db1f"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 569 entries, 0 to 568\n","Data columns (total 33 columns):\n"," #   Column                   Non-Null Count  Dtype  \n","---  ------                   --------------  -----  \n"," 0   id                       569 non-null    int64  \n"," 1   diagnosis                569 non-null    object \n"," 2   radius_mean              569 non-null    float64\n"," 3   texture_mean             569 non-null    float64\n"," 4   perimeter_mean           569 non-null    float64\n"," 5   area_mean                569 non-null    float64\n"," 6   smoothness_mean          569 non-null    float64\n"," 7   compactness_mean         569 non-null    float64\n"," 8   concavity_mean           569 non-null    float64\n"," 9   concave points_mean      569 non-null    float64\n"," 10  symmetry_mean            569 non-null    float64\n"," 11  fractal_dimension_mean   569 non-null    float64\n"," 12  radius_se                569 non-null    float64\n"," 13  texture_se               569 non-null    float64\n"," 14  perimeter_se             569 non-null    float64\n"," 15  area_se                  569 non-null    float64\n"," 16  smoothness_se            569 non-null    float64\n"," 17  compactness_se           569 non-null    float64\n"," 18  concavity_se             569 non-null    float64\n"," 19  concave points_se        569 non-null    float64\n"," 20  symmetry_se              569 non-null    float64\n"," 21  fractal_dimension_se     569 non-null    float64\n"," 22  radius_worst             569 non-null    float64\n"," 23  texture_worst            569 non-null    float64\n"," 24  perimeter_worst          569 non-null    float64\n"," 25  area_worst               569 non-null    float64\n"," 26  smoothness_worst         569 non-null    float64\n"," 27  compactness_worst        569 non-null    float64\n"," 28  concavity_worst          569 non-null    float64\n"," 29  concave points_worst     569 non-null    float64\n"," 30  symmetry_worst           569 non-null    float64\n"," 31  fractal_dimension_worst  569 non-null    float64\n"," 32  Unnamed: 32              0 non-null      float64\n","dtypes: float64(31), int64(1), object(1)\n","memory usage: 146.8+ KB\n"]}]},{"cell_type":"code","source":["# Eliminemos las columnas id y Unnamed: 32\n","df = df.drop([\"id\", \"Unnamed: 32\"], axis = 1)\n","df.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BqXeiCurJqHy","executionInfo":{"status":"ok","timestamp":1755788560253,"user_tz":300,"elapsed":25,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"c7b6bf4f-376f-4f29-e401-69019565c716"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 569 entries, 0 to 568\n","Data columns (total 31 columns):\n"," #   Column                   Non-Null Count  Dtype  \n","---  ------                   --------------  -----  \n"," 0   diagnosis                569 non-null    object \n"," 1   radius_mean              569 non-null    float64\n"," 2   texture_mean             569 non-null    float64\n"," 3   perimeter_mean           569 non-null    float64\n"," 4   area_mean                569 non-null    float64\n"," 5   smoothness_mean          569 non-null    float64\n"," 6   compactness_mean         569 non-null    float64\n"," 7   concavity_mean           569 non-null    float64\n"," 8   concave points_mean      569 non-null    float64\n"," 9   symmetry_mean            569 non-null    float64\n"," 10  fractal_dimension_mean   569 non-null    float64\n"," 11  radius_se                569 non-null    float64\n"," 12  texture_se               569 non-null    float64\n"," 13  perimeter_se             569 non-null    float64\n"," 14  area_se                  569 non-null    float64\n"," 15  smoothness_se            569 non-null    float64\n"," 16  compactness_se           569 non-null    float64\n"," 17  concavity_se             569 non-null    float64\n"," 18  concave points_se        569 non-null    float64\n"," 19  symmetry_se              569 non-null    float64\n"," 20  fractal_dimension_se     569 non-null    float64\n"," 21  radius_worst             569 non-null    float64\n"," 22  texture_worst            569 non-null    float64\n"," 23  perimeter_worst          569 non-null    float64\n"," 24  area_worst               569 non-null    float64\n"," 25  smoothness_worst         569 non-null    float64\n"," 26  compactness_worst        569 non-null    float64\n"," 27  concavity_worst          569 non-null    float64\n"," 28  concave points_worst     569 non-null    float64\n"," 29  symmetry_worst           569 non-null    float64\n"," 30  fractal_dimension_worst  569 non-null    float64\n","dtypes: float64(30), object(1)\n","memory usage: 137.9+ KB\n"]}]},{"cell_type":"code","source":["# Numero de valores faltantes para todo el dataframe\n","df.isnull().sum().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X73sUlxYJ8Og","executionInfo":{"status":"ok","timestamp":1755788630901,"user_tz":300,"elapsed":16,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"10a0ad3d-a31f-48cb-f323-59350f16f18b"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["np.int64(0)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# Que valores compone a la variable dependiente :\n","df.diagnosis.unique()\n","# M : maligno\n","# B : Benigno\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XtHniOruKMmY","executionInfo":{"status":"ok","timestamp":1755788695973,"user_tz":300,"elapsed":11,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"53101b2a-9cfa-4232-9c10-9e776ddebe25"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['M', 'B'], dtype=object)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# Podriamos considerar como una estratregia adecuada (basada en nuestra explicacion teorica)\n","# de traducir la informacion cualitativa de la variable dependiente a informacion\n","# cuantitativa\n","\n","# Resultado de un debate sumamente productivo, reconocemos 3 posibles escenarios de transformacion\n"],"metadata":{"id":"ZJPE4GlpKl1S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Primer escenario : B=0 M=1\n","# Segundo escenario : B=0 M=-1\n","# Tercer escenario : B=1 M=-1"],"metadata":{"id":"wOHQyrBqMHWz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Consideremos para nuestro modelo base el primer escenario : B=0 -  M=1"],"metadata":{"id":"mRdxkp3YMYTr"}},{"cell_type":"code","source":["# Veamos la distribucion de valores dentro de la variable dependiente\n","df.diagnosis.value_counts()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":178},"id":"0vw0Ut4eNRo6","executionInfo":{"status":"ok","timestamp":1755789494880,"user_tz":300,"elapsed":31,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"ab8dcd0b-0533-4372-c64f-e4e875b86709"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["diagnosis\n","B    357\n","M    212\n","Name: count, dtype: int64"],"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>count</th>\n","    </tr>\n","    <tr>\n","      <th>diagnosis</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>B</th>\n","      <td>357</td>\n","    </tr>\n","    <tr>\n","      <th>M</th>\n","      <td>212</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> int64</label>"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# Definamos las variables independientes\n","X = df.drop(\"diagnosis\", axis = 1)\n","\n","# Definamos a la variable dependientes (Primer escenario)\n","y = df.diagnosis.replace({\"B\":0, \"M\":1})\n","\n","# Particionado de datos : subconjunto de entrenamiento y subconjunto de testeo\n","X_train, X_test, y_train, y_test = train_test_split(X,y,train_size= 0.93,\n","                                                    # Muestreo estratificado\n","                                                    stratify= y\n","                                                    )\n","\n","# Instanciamos la clase a modelar\n","model_base_reglog = LogisticRegression(max_iter=2000*5)\n","\n","# Adjuntamos el modelo con el subconjunto de entrenamiento\n","model_base_reglog.fit(X_train,y_train)\n","\n","# COnstruccion de pronosticos para el subconjunto de testeo\n","y_test_pred = model_base_reglog.predict(X_test)\n","\n","# Calculamos un indicador de calidad : Exactitud\n","metrics.accuracy_score(y_test, y_test_pred)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oCCaVxpmMenx","executionInfo":{"status":"ok","timestamp":1755790501479,"user_tz":300,"elapsed":510,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"3459209a-96f4-4187-d3eb-2f95aaf6e1ab"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1615373770.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  y = df.diagnosis.replace({\"B\":0, \"M\":1})\n"]},{"output_type":"execute_result","data":{"text/plain":["0.975"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["# Construyamos un segundo modelo para nuestro Segundo escenario : B=0 - M=-1\n"],"metadata":{"id":"QB0vRIDpP2Ei"}},{"cell_type":"code","source":["# Definamos las variables independientes\n","X = df.drop(\"diagnosis\", axis = 1)\n","\n","# Definamos a la variable dependientes (Primer escenario)\n","y = df.diagnosis.replace({\"B\":0, \"M\":-1})\n","\n","# Particionado de datos : subconjunto de entrenamiento y subconjunto de testeo\n","X_train, X_test, y_train, y_test = train_test_split(X,y,train_size= 0.93,\n","                                                    # Muestreo estratificado\n","                                                    stratify= y\n","                                                    )\n","\n","# Instanciamos la clase a modelar\n","model_base_reglog = LogisticRegression(max_iter=2000*5)\n","\n","# Adjuntamos el modelo con el subconjunto de entrenamiento\n","model_base_reglog.fit(X_train,y_train)\n","\n","# COnstruccion de pronosticos para el subconjunto de testeo\n","y_test_pred = model_base_reglog.predict(X_test)\n","\n","# Calculamos un indicador de calidad : Exactitud\n","metrics.accuracy_score(y_test, y_test_pred)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rB4KBjqxP9PC","executionInfo":{"status":"ok","timestamp":1755790512407,"user_tz":300,"elapsed":2369,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"c84bf0e7-cbcc-4b71-d058-6b294d732494"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2849747194.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  y = df.diagnosis.replace({\"B\":0, \"M\":-1})\n"]},{"output_type":"execute_result","data":{"text/plain":["0.925"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["# Construyamos un tercer modelo para nuestro Tercer escenario : B=1 - M=-1"],"metadata":{"id":"FcRKFMpbQKmJ"}},{"cell_type":"code","source":["# Definamos las variables independientes\n","X = df.drop(\"diagnosis\", axis = 1)\n","\n","# Definamos a la variable dependientes (Primer escenario)\n","y = df.diagnosis.replace({\"B\":1, \"M\":-1})\n","\n","# Particionado de datos : subconjunto de entrenamiento y subconjunto de testeo\n","X_train, X_test, y_train, y_test = train_test_split(X,y,train_size= 0.93,\n","                                                    # Muestreo estratificado\n","                                                    stratify= y\n","                                                    )\n","\n","# Instanciamos la clase a modelar\n","model_base_reglog = LogisticRegression(max_iter=2000*5)\n","\n","# Adjuntamos el modelo con el subconjunto de entrenamiento\n","model_base_reglog.fit(X_train,y_train)\n","\n","# COnstruccion de pronosticos para el subconjunto de testeo\n","y_test_pred = model_base_reglog.predict(X_test)\n","\n","# Calculamos un indicador de calidad : Exactitud\n","metrics.accuracy_score(y_test, y_test_pred)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HSJk3y9-QP7K","executionInfo":{"status":"ok","timestamp":1755790518197,"user_tz":300,"elapsed":809,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"1c4f371b-91fc-4492-8d38-fd4ece594221"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-792022246.py:5: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  y = df.diagnosis.replace({\"B\":1, \"M\":-1})\n"]},{"output_type":"execute_result","data":{"text/plain":["0.975"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","source":["# Implementemos una estrategia adecuada de comparacion de modelos\n"],"metadata":{"id":"J62ykIEYRmGa"}},{"cell_type":"code","source":["# Fijemos los datos de entrenamiento y testeo para poder comparar las\n","# divarsas estrategias formuladas\n","\n","\n","# Primera Estrategia :\n","# Definamos a la variable dependientes (Primer escenario)\n","y1 = df.diagnosis.replace({\"B\":0, \"M\":1})\n","\n","# Particionado de datos : subconjunto de entrenamiento y subconjunto de testeo\n","X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X,y1,train_size= 0.93,\n","                                                    # Muestreo estratificado\n","                                                    stratify= y1\n","                                                    )\n","\n","# Segunda Estrategia :\n","y2 = df.diagnosis.replace({\"B\":0, \"M\":-1})\n","\n","# Particionado de datos : subconjunto de entrenamiento y subconjunto de testeo\n","X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X,y2,train_size= 0.93,\n","                                                    # Muestreo estratificado\n","                                                    stratify= y2\n","                                                    )\n","\n","# Tercera Estrategia\n","y3 = df.diagnosis.replace({\"B\":1, \"M\":-1})\n","\n","# Particionado de datos : subconjunto de entrenamiento y subconjunto de testeo\n","X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X,y3,train_size= 0.93,\n","                                                    # Muestreo estratificado\n","                                                    stratify= y3\n","                                                    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JMTduQS8RpiK","executionInfo":{"status":"ok","timestamp":1755790808301,"user_tz":300,"elapsed":33,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"b6ac8bc7-721f-45af-bdec-2293395ef556"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1570778886.py:7: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  y1 = df.diagnosis.replace({\"B\":0, \"M\":1})\n","/tmp/ipython-input-1570778886.py:16: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  y2 = df.diagnosis.replace({\"B\":0, \"M\":-1})\n","/tmp/ipython-input-1570778886.py:25: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  y3 = df.diagnosis.replace({\"B\":1, \"M\":-1})\n"]}]},{"cell_type":"code","source":["# MOdelo para la primera estrategia\n","model_base_y1 = LogisticRegression(max_iter=10000).fit(X_train_1,y_train_1 )\n","exactitud1 = metrics.accuracy_score(y_test_1, model_base_y1.predict(X_test_1))\n","\n","# MOdelo para la segunda estrategia\n","model_base_y2 = LogisticRegression(max_iter=10000).fit(X_train_2,y_train_2 )\n","exactitud2 = metrics.accuracy_score(y_test_2, model_base_y2.predict(X_test_2))\n","\n","# MOdelo para la tercera estrategia\n","model_base_y3 = LogisticRegression(max_iter=10000).fit(X_train_3,y_train_3 )\n","exactitud3 = metrics.accuracy_score(y_test_3, model_base_y3.predict(X_test_3))\n","\n","# Resumir los resultados\n","print(\"\"\"\n","Exactitud (modelo1) : %f\n","Exactitud (modelo2) : %f\n","Exactitud (modelo3) : %f\n","\"\"\" %(exactitud1,exactitud2,exactitud3))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PK_CSTnRzQa","executionInfo":{"status":"ok","timestamp":1755791062897,"user_tz":300,"elapsed":1796,"user":{"displayName":"Abraham Zamudio","userId":"17856795790189684981"}},"outputId":"ee002e4c-01d3-4cff-e07a-f0bf13a4748a"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Exactitud (modelo1) : 0.950000\n","Exactitud (modelo2) : 0.950000\n","Exactitud (modelo3) : 0.925000\n","\n"]}]}]}